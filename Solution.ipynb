{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "import sklearn \n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 5000 # Максимальное кол-во признаков-слов для CountVectorizer\n",
    "CAT_COLS = ['category', 'subcategory'] # факторизуемые колонки\n",
    "TARGET_COLUMNS = ['title', 'description', 'attrs', ['title', 'description']] # колонки, для построения BagOfWords тиблиц\n",
    "SEED = 8451 # Показатель рандома"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame.from_csv('data/avito_train.tsv', sep='\\t')\n",
    "test_data = pd.DataFrame.from_csv('data/avito_test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itemid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000010</th>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category            subcategory              title  \\\n",
       "itemid                                                          \n",
       "10000010  Транспорт  Автомобили с пробегом  Toyota Sera, 1991   \n",
       "\n",
       "                                                description  \\\n",
       "itemid                                                        \n",
       "10000010  Новая оригинальная линзованая оптика на ксенон...   \n",
       "\n",
       "                                                      attrs   price  \\\n",
       "itemid                                                                \n",
       "10000010  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...  150000   \n",
       "\n",
       "          is_proved  is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "itemid                                                                          \n",
       "10000010        NaN           0           0           0         0         0.03  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = train_data[:10000]\n",
    "test_data = test_data[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category             int64         10\n",
      "subcategory          int64         53\n",
      "title                object       8542\n",
      "description          object       9842\n",
      "attrs                object       4405\n",
      "price                int64       1344\n",
      "is_proved            float64          3\n",
      "is_blocked           int64          2\n",
      "phones_cnt           int64          8\n",
      "emails_cnt           int64          3\n",
      "urls_cnt             int64          6\n",
      "close_hours          float64       2124\n"
     ]
    }
   ],
   "source": [
    "# просматриваем информацию в колонках\n",
    "for column in train_data.columns:\n",
    "    print(\"{: <20} {:} {: >10}\".format(column, train[column].dtype, len(train[column].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ".# по таблице определяем категориальные string колонки\n",
    "cat_cols = ['category','subcategory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "enc = LabelEncoder()\n",
    "train[cat_cols] = train[cat_cols].apply(lambda x: pd.factorize(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load scripts/preprocessing.py\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# стемминг и знаки пунктуации\n",
    "stemmer = RussianStemmer()\n",
    "exclude = string.punctuation + string.digits\n",
    "stopwords = set(stopwords.words(\"russian\"))\n",
    "\n",
    "vectorizer = None\n",
    "\n",
    "# Преобразование строки в массив слов со стеммингом и lower()\n",
    "def clear(text):\n",
    "    # pre. Проверка на нули в данных\n",
    "    text = str(text)\n",
    "    if text == \"nan\":\n",
    "        return []\n",
    "    # 1. Убираем не-буквы\n",
    "    temp = re.sub(\"[^a-zA-Z|^а-яА-Я]\", \" \", text)\n",
    "    # 2. Преобразуем в прописные и делим по словам\n",
    "    temp = temp.lower().split()\n",
    "    # 3. Стемминг и уборка стоп-слов\n",
    "    temp = [stemmer.stem(i) for i in temp if i not in stopwords]\n",
    "    temp = [i for i in temp if len(i) > 2]\n",
    "    return temp\n",
    "\n",
    "# Создание таблицы BagOfWords из колонки\n",
    "def frame(df, column):\n",
    "    global vectorizer;\n",
    "    print(\"COLUMN: {0}\".format(column))\n",
    "    # 1. Получаем очищенные данные и представляем строчкой\n",
    "    cleared = []\n",
    "    if type(column) is str: # обработка одной колонки\n",
    "        cleared = [\" \".join(clear(i)) for i in df[column]]\n",
    "    else: # обработка 2 колонок\n",
    "        temp = [series_.values for id_, series_ in df[column].iterrows()]\n",
    "        temp = [\" \".join(clear(str(i) + str(j))) for i,j in temp]\n",
    "        cleared = cleared + temp\n",
    "    print(\"- Cleared\")\n",
    "    # 2. Создаём CountVectorizer - подсчёта слов\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                                 tokenizer=None,\n",
    "                                 preprocessor=None,\n",
    "                                 stop_words=None,\n",
    "                                 max_features=MAX_FEATURES)\n",
    "    print(\"- Words extracted\")\n",
    "    # 3. Учим словарю и обрабатываем\n",
    "    features = vectorizer.fit_transform(cleared)\n",
    "    print(\"- Processed\\n\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMN: title\n",
      "- Cleared\n",
      "- Words extracted\n",
      "- Processed\n",
      "\n",
      "COLUMN: description\n",
      "- Cleared\n",
      "- Words extracted\n",
      "- Processed\n",
      "\n",
      "COLUMN: attrs\n",
      "- Cleared\n",
      "- Words extracted\n",
      "- Processed\n",
      "\n",
      "COLUMN: ['title', 'description']\n",
      "- Cleared\n",
      "- Words extracted\n",
      "- Processed\n",
      "\n",
      "COLUMN: title\n",
      "- Cleared\n",
      "- Words extracted\n",
      "- Processed\n",
      "\n",
      "COLUMN: description\n",
      "- Cleared\n",
      "- Words extracted\n",
      "- Processed\n",
      "\n",
      "COLUMN: attrs\n",
      "- Cleared\n",
      "- Words extracted\n",
      "- Processed\n",
      "\n",
      "COLUMN: ['title', 'description']\n",
      "- Cleared\n",
      "- Words extracted\n",
      "- Processed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### Полный процесс предобработки данных\n",
    "###\n",
    "\n",
    "\n",
    "# 1. Факторизация категориальных данных\n",
    "train_data[CAT_COLS] = train_data[CAT_COLS].apply(lambda x: pd.factorize(x)[0])\n",
    "\n",
    "test_data[CAT_COLS] = test_data[CAT_COLS].apply(lambda x: pd.factorize(x)[0])\n",
    "\n",
    "# 2. Получение матриц BagOgWords\n",
    "sparse_frames_train = [frame(train_data, i) for i in TARGET_COLUMNS]\n",
    "sparse_frames_test = [frame(test_data, i) for i in TARGET_COLUMNS]\n",
    "\n",
    "# 3. Обучаем модель_1[] (модели) для каждой матрицы\n",
    "\n",
    "\n",
    "# 4. Делаем предсказания модель_1[] каждой матрицы\n",
    "\n",
    "# 5. Делаем сводную матрицу и добавляем столбцы категорий+доп.данных\n",
    "\n",
    "# 7. По сводной матрице обучаем модель_2\n",
    "\n",
    "# 8. По сводной матрице делаем предсказание модель_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "temp = pd.DataFrame(sparse_frames[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Started:  title\n",
      "##### Started:  description\n"
     ]
    }
   ],
   "source": [
    "params = list()\n",
    "from scipy.stats import randint\n",
    "\n",
    "forest=RandomForestClassifier(n_estimators=10, random_state=SEED)\n",
    "param_grid = {\"max_depth\": randint(low=1, high=15),\n",
    "              \"max_features\": ['sqrt', 'log2'],\n",
    "              \"min_samples_leaf\": [4, 8, 16, 32, 64, 128],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "#              }\n",
    "i = 0\n",
    "for fr in sparse_frames_train[0: 2]:\n",
    "    print(\"##### Started: \", TARGET_COLUMNS[i])\n",
    "    i += 1\n",
    "    X = pd.DataFrame(fr.toarray())\n",
    "    y = df.is_blocked\n",
    "    grid_search = RandomizedSearchCV(forest, param_distributions=param_grid, \n",
    "                                     n_iter=15, cv=5, scoring='neg_mean_squared_error', random_state=45426)\n",
    "    grid_search.fit(X, y)\n",
    "    params.append(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bootstrap': True,\n",
       "  'criterion': 'entropy',\n",
       "  'max_depth': 11,\n",
       "  'max_features': 'sqrt',\n",
       "  'min_samples_leaf': 128},\n",
       " {'bootstrap': True,\n",
       "  'criterion': 'entropy',\n",
       "  'max_depth': 9,\n",
       "  'max_features': 'sqrt',\n",
       "  'min_samples_leaf': 8}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "predictions = []\n",
    "\n",
    "for fr in sparse_frames[0: 1]:  \n",
    "    X = pd.DataFrame(fr.toarray())\n",
    "    y = df.is_blocked\n",
    "    sgd_clf.fit(X, y)\n",
    "    predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.95152424,  0.96051974,  0.959     ,  0.95947974,  0.95997999])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf.predict_proba()\n",
    "# scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "# scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=9322/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_test = pd.DataFrame.from_csv('data/avito_test.tsv', sep='\\t')\n",
    "data_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./scripts')\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_title = pd.DataFrame(preprocessing.frame(train,\"title\").toarray())\n",
    "df_desc = pd.DataFrame(preprocessing.frame(train,\"description\").toarray())\n",
    "df_attrs = pd.DataFrame(preprocessing.frame(train,\"attrs\").toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
