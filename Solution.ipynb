{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "import sklearn \n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "\n",
    "MAX_FEATURES = 5000 # Максимальное кол-во признаков-слов для CountVectorizer\n",
    "CAT_COLS = ['category', 'subcategory'] # факторизуемые колонки\n",
    "TARGET_COLUMNS = ['title', 'description', 'attrs', ['title', 'description']] # колонки, для построения BagOfWords тиблиц\n",
    "SEED = 8451 # Показатель рандома\n",
    "MODEL_COLUMNS = ['price', 'phones_cnt', 'emails_cnt', 'urls_cnt', 'category', 'subcategory']\n",
    "FOREST_TRAIN_PARAMETERS = {\"max_depth\": randint(low=1, high=15),\n",
    "                  \"max_features\": ['sqrt', 'log2'],\n",
    "                  \"min_samples_leaf\": [4, 8, 16, 32],\n",
    "                  \"bootstrap\": [True, False],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"],\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_raw = pd.DataFrame.from_csv('data/avito_train.tsv', sep='\\t')\n",
    "test_data_raw = pd.DataFrame.from_csv('data/avito_test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3995803, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itemid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000010</th>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category            subcategory              title  \\\n",
       "itemid                                                          \n",
       "10000010  Транспорт  Автомобили с пробегом  Toyota Sera, 1991   \n",
       "\n",
       "                                                description  \\\n",
       "itemid                                                        \n",
       "10000010  Новая оригинальная линзованая оптика на ксенон...   \n",
       "\n",
       "                                                      attrs   price  \\\n",
       "itemid                                                                \n",
       "10000010  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...  150000   \n",
       "\n",
       "          is_proved  is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "itemid                                                                          \n",
       "10000010        NaN           0           0           0         0         0.03  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = train_data_raw[:50000]\n",
    "test_data = test_data_raw[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    ".# просматриваем информацию в колонках\n",
    "for column in train_data.columns:\n",
    "    print(\"{: <20} {:} {: >10}\".format(column, train[column].dtype, len(train[column].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ".# по таблице определяем категориальные string колонки\n",
    "cat_cols = ['category','subcategory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "enc = LabelEncoder()\n",
    "train[cat_cols] = train[cat_cols].apply(lambda x: pd.factorize(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load scripts/preprocessing.py\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.stats import randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# стемминг и знаки пунктуации\n",
    "stemmer = RussianStemmer()\n",
    "exclude = string.punctuation + string.digits\n",
    "stopwords = set(stopwords.words(\"russian\"))\n",
    "\n",
    "vectorizer = None\n",
    "\n",
    "# Преобразование строки в массив слов со стеммингом и lower()\n",
    "def clear(text):\n",
    "    # pre. Проверка на нули в данных\n",
    "    text = str(text)\n",
    "    if text == \"nan\":\n",
    "        return []\n",
    "    # 1. Убираем не-буквы\n",
    "    temp = re.sub(\"[^a-zA-Z|^а-яА-Я]\", \" \", text)\n",
    "    # 2. Преобразуем в прописные и делим по словам\n",
    "    temp = temp.lower().split()\n",
    "    # 3. Стемминг и уборка стоп-слов\n",
    "    temp = [i for i in temp if i not in stopwords and len(i) > 2]\n",
    "#     temp = [i for i in temp if len(i) > 2]\n",
    "    return temp\n",
    "\n",
    "def preprocessFeatures(df, vectorizers=None, columns=TARGET_COLUMNS, max_features=MAX_FEATURES):\n",
    "    data_type = -1; # -1: smth wrong; 0: тренировочные данные; 1: тестовые данные\n",
    "    if vectorizers == None:\n",
    "        print(\"PROCESSING TRAIN DATA\")\n",
    "        vectorizers = dict() # полученные вектрорайзеры (только для тренировочной выборки)\n",
    "        data_type = 0\n",
    "    else:\n",
    "        print(\"PROCESSING TEST DATA\")\n",
    "        data_type = 1\n",
    "    features = [] # результат предпроцессинга\n",
    "    for column in columns:\n",
    "        print(\"COLUMN: {0}\".format(column))\n",
    "        # 1. Получаем очищенные данные и представляем строчкой\n",
    "        cleared = [] # список очищенных и преобразованных строк\n",
    "        if type(column) is str: # обработка одной колонки\n",
    "            cleared = [\" \".join(clear(i)) for i in df[column]]\n",
    "        else: # обработка 2 колонок\n",
    "            temp = [series_.values for id_, series_ in df[column].iterrows()]\n",
    "            temp = [\" \".join(clear(str(i) + str(j))) for i,j in temp]\n",
    "            cleared = cleared + temp\n",
    "        print(\"  - Cleared\")\n",
    "        # 2. Если данные тестовые - то vectorizer для колонки уже есть, используем его. Если тренировочные - создаём.\n",
    "        if data_type == 0:\n",
    "            vect = CountVectorizer(analyzer=\"word\",\n",
    "                                         tokenizer=None,\n",
    "                                         preprocessor=None,\n",
    "                                         stop_words=None)\n",
    "            # 3a. Учим словарю и обрабатываем\n",
    "            features.append(vect.fit_transform(cleared)) # обучаем + преобразуем\n",
    "            vectorizers[\"\".join(column)] = vect # запоминаем получившийся векторайзер\n",
    "        else:\n",
    "            # 3b. Просто обрабатываем данные\n",
    "            features.append(vectorizers[\"\".join(column)].transform(cleared)) # просто преобразем\n",
    "        print(\"  - Processed\\n\")\n",
    "    return features, vectorizers\n",
    "\n",
    "# Делает предсказания для ряда BagOfWords матриц на ряде моделей\n",
    "def modelsPredicts(frames, models):\n",
    "    predictions = []\n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        X = frames[i]\n",
    "        y = model.predict(X)\n",
    "        predictions.append(y)\n",
    "    return predictions\n",
    "\n",
    "# Добавление не использованных данных + выделение меток\n",
    "def concatenateRemaining(df, predictions, model_columns = MODEL_COLUMNS, cat_cols=CAT_COLS):\n",
    "    # 1. Факторизация категориальных данных\n",
    "    df[cat_cols] = df[cat_cols].apply(lambda x: pd.factorize(x)[0])\n",
    "    X = pd.DataFrame(predictions).T\n",
    "    X = X.set_index(df.index)\n",
    "    for column in MODEL_COLUMNS:\n",
    "        X[column] = df[column]\n",
    "    return X\n",
    "\n",
    "# Вычисление лучших параметров для покрывающей модели (над ост. признаками и результатами выч.)\n",
    "def getCoveringMovelParams(X,y,\n",
    "                       parameters=FOREST_TRAIN_PARAMETERS,\n",
    "                       seed=SEED, \n",
    "                       model=RandomForestClassifier(random_state=SEED),\n",
    "                      ):\n",
    "    grid_search = RandomizedSearchCV(model, \n",
    "                                     param_distributions=parameters, \n",
    "                                     n_iter=2, cv=5, \n",
    "                                     scoring='neg_mean_squared_error', \n",
    "                                     random_state=SEED, \n",
    "                                     verbose = 1)\n",
    "    grid_search.fit(X,y)\n",
    "    return grid_search.best_params_\n",
    "\n",
    "# Обучение локальных моделей, покрывающей модели, векторайзера\n",
    "def trainModel(df, cat_cols=CAT_COLS, max_features=MAX_FEATURES, target_columns=TARGET_COLUMNS, seed=SEED):\n",
    "    # 1. Получение матриц BagOgWords\n",
    "    sparse_frames, vectorizers = preprocessFeatures(df)\n",
    "    # 2. Обучаем модель_1[] (модели) для каждой матрицы\n",
    "    models = []\n",
    "    print(\"FITTING LOCAL MODELS\")\n",
    "    i = 1\n",
    "    for fr in sparse_frames:\n",
    "        sgd_clf = SGDRegressor(random_state=seed)\n",
    "#         X = pd.DataFrame(fr.toarray())\n",
    "        X = fr\n",
    "        y = df.is_blocked\n",
    "        sgd_clf.fit(X, y)\n",
    "        models.append(sgd_clf)\n",
    "        print(\"  -fitted {0} from {1}\".format(i, len(sparse_frames)))\n",
    "        i = i + 1;\n",
    "    print()\n",
    "    # 3. Делаем предсказания модель_1[] каждой матрицы\n",
    "    predictions = modelsPredicts(sparse_frames, models)\n",
    "    # 4. Делаем сводную матрицу и добавляем столбцы категорий+доп.данных\n",
    "    X = concatenateRemaining(df, predictions)\n",
    "    y = df['is_blocked']\n",
    "    # 5. По сводной матрице обучаем модель_2\n",
    "    best_params = getCoveringMovelParams(X,y)\n",
    "    print(best_params)\n",
    "    covering_model = RandomForestClassifier(**best_params, n_jobs=-1).fit(X,y)\n",
    "    return vectorizers, models, covering_model, sparse_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "### Полный процесс предобработки данных\n",
    "###\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.stats import randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "train_data = pd.DataFrame.from_csv('data/avito_train.tsv', sep='\\t')\n",
    "test_data = pd.DataFrame.from_csv('data/avito_test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING TRAIN DATA\n",
      "COLUMN: title\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: description\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: attrs\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: ['title', 'description']\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "FITTING LOCAL MODELS\n",
      "  -fitted 1 from 4\n",
      "  -fitted 2 from 4\n",
      "  -fitted 3 from 4\n",
      "  -fitted 4 from 4\n",
      "\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 14.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 4, 'criterion': 'entropy', 'max_depth': 9, 'bootstrap': True, 'max_features': 'log2'}\n",
      "PROCESSING TEST DATA\n",
      "COLUMN: title\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: description\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: attrs\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: ['title', 'description']\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = train_data\n",
    "test = test_data\n",
    "\n",
    "# 1. Обучение модели\n",
    "vectorizers, models, covering_model, sparse_frames = trainModel(train)\n",
    "\n",
    "# 2. Обработка данных\n",
    "pre_data = preprocessFeatures(test, vectorizers)[0]\n",
    "\n",
    "# 3. Предсказания частных моделей\n",
    "priv_predictions = modelsPredicts(pre_data,models)\n",
    "\n",
    "# 4. Получение сводной матрицы\n",
    "X = concatenateRemaining(test, priv_predictions)\n",
    "\n",
    "# 5. Предсказание покрывающей модели\n",
    "test_predictions = covering_model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(test_predictions)\n",
    "result.index = test.index\n",
    "result = result.sort_values(by=1, ascending=False)\n",
    "result = result.drop([0], axis=1)\n",
    "\n",
    "result_final = pd.DataFrame(result.index)\n",
    "result_final.to_csv('final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "temp = pd.DataFrame(sparse_frames[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Started:  title\n",
      "##### Started:  description\n"
     ]
    }
   ],
   "source": [
    "params = list()\n",
    "from scipy.stats import randint\n",
    "\n",
    "forest=RandomForestClassifier(n_estimators=10, random_state=SEED)\n",
    "param_grid = {\"max_depth\": randint(low=1, high=15),\n",
    "              \"max_features\": ['sqrt', 'log2'],\n",
    "              \"min_samples_leaf\": [4, 8, 16, 32, 64, 128],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "#              }\n",
    "i = 0\n",
    "for fr in sparse_frames_train[0: 2]:\n",
    "    print(\"##### Started: \", TARGET_COLUMNS[i])\n",
    "    i += 1\n",
    "    X = pd.DataFrame(fr.toarray())\n",
    "    y = df.is_blocked\n",
    "    grid_search = RandomizedSearchCV(forest, param_distributions=param_grid, \n",
    "                                     n_iter=15, cv=5, scoring='neg_mean_squared_error', random_state=45426)\n",
    "    grid_search.fit(X, y)\n",
    "    params.append(grid_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
