{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "import sklearn \n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "\n",
    "MAX_FEATURES = 5000 # Максимальное кол-во признаков-слов для CountVectorizer\n",
    "CAT_COLS = ['category', 'subcategory'] # факторизуемые колонки\n",
    "TARGET_COLUMNS = ['title', 'description', 'attrs', ['title', 'description']] # колонки, для построения BagOfWords тиблиц\n",
    "SEED = 8451 # Показатель рандома\n",
    "MODEL_COLUMNS = ['price', 'phones_cnt', 'emails_cnt', 'urls_cnt', 'category', 'subcategory']\n",
    "FOREST_TRAIN_PARAMETERS = {\"max_depth\": randint(low=1, high=15),\n",
    "                  \"max_features\": ['sqrt', 'log2'],\n",
    "                  \"min_samples_leaf\": [4, 8, 16, 32],\n",
    "                  \"bootstrap\": [True, False],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"],\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_raw = pd.DataFrame.from_csv('data/avito_train.tsv', sep='\\t')\n",
    "test_data_raw = pd.DataFrame.from_csv('data/avito_test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3995803, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itemid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000010</th>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category            subcategory              title  \\\n",
       "itemid                                                          \n",
       "10000010  Транспорт  Автомобили с пробегом  Toyota Sera, 1991   \n",
       "\n",
       "                                                description  \\\n",
       "itemid                                                        \n",
       "10000010  Новая оригинальная линзованая оптика на ксенон...   \n",
       "\n",
       "                                                      attrs   price  \\\n",
       "itemid                                                                \n",
       "10000010  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...  150000   \n",
       "\n",
       "          is_proved  is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "itemid                                                                          \n",
       "10000010        NaN           0           0           0         0         0.03  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_raw.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = train_data_raw[:10000]\n",
    "test_data = test_data_raw[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    ".# просматриваем информацию в колонках\n",
    "for column in train_data.columns:\n",
    "    print(\"{: <20} {:} {: >10}\".format(column, train[column].dtype, len(train[column].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ".# по таблице определяем категориальные string колонки\n",
    "cat_cols = ['category','subcategory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "enc = LabelEncoder()\n",
    "train[cat_cols] = train[cat_cols].apply(lambda x: pd.factorize(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load scripts/preprocessing.py\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.stats import randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# стемминг и знаки пунктуации\n",
    "stemmer = RussianStemmer()\n",
    "exclude = string.punctuation + string.digits\n",
    "stopwords = set(stopwords.words(\"russian\"))\n",
    "\n",
    "vectorizer = None\n",
    "\n",
    "# Преобразование строки в массив слов со стеммингом и lower()\n",
    "def clear(text):\n",
    "    # pre. Проверка на нули в данных\n",
    "    text = str(text)\n",
    "    if text == \"nan\":\n",
    "        return []\n",
    "    # 1. Убираем не-буквы\n",
    "    temp = re.sub(\"[^a-zA-Z|^а-яА-Я]\", \" \", text)\n",
    "    # 2. Преобразуем в прописные и делим по словам\n",
    "    temp = temp.lower().split()\n",
    "    # 3. Стемминг и уборка стоп-слов\n",
    "    temp = [stemmer.stem(i) for i in temp if i not in stopwords]\n",
    "    temp = [i for i in temp if len(i) > 2]\n",
    "    return temp\n",
    "\n",
    "def preprocessFeatures(df, vectorizers=None, columns=TARGET_COLUMNS, max_features=MAX_FEATURES):\n",
    "    data_type = -1; # -1: smth wrong; 0: тренировочные данные; 1: тестовые данные\n",
    "    if vectorizers == None:\n",
    "        print(\"PROCESSING TRAIN DATA\")\n",
    "        vectorizers = dict() # полученные вектрорайзеры (только для тренировочной выборки)\n",
    "        data_type = 0\n",
    "    else:\n",
    "        print(\"PROCESSING TEST DATA\")\n",
    "        data_type = 1\n",
    "    features = [] # результат предпроцессинга\n",
    "    for column in columns:\n",
    "        print(\"COLUMN: {0}\".format(column))\n",
    "        # 1. Получаем очищенные данные и представляем строчкой\n",
    "        cleared = [] # список очищенных и преобразованных строк\n",
    "        if type(column) is str: # обработка одной колонки\n",
    "            cleared = [\" \".join(clear(i)) for i in df[column]]\n",
    "        else: # обработка 2 колонок\n",
    "            temp = [series_.values for id_, series_ in df[column].iterrows()]\n",
    "            temp = [\" \".join(clear(str(i) + str(j))) for i,j in temp]\n",
    "            cleared = cleared + temp\n",
    "        print(\"  - Cleared\")\n",
    "        # 2. Если данные тестовые - то vectorizer для колонки уже есть, используем его. Если тренировочные - создаём.\n",
    "        if data_type == 0:\n",
    "            vect = CountVectorizer(analyzer=\"word\",\n",
    "                                         tokenizer=None,\n",
    "                                         preprocessor=None,\n",
    "                                         stop_words=None,\n",
    "                                         max_features=max_features)\n",
    "            # 3a. Учим словарю и обрабатываем\n",
    "            features.append(vect.fit_transform(cleared)) # обучаем + преобразуем\n",
    "            vectorizers[\"\".join(column)] = vect # запоминаем получившийся векторайзер\n",
    "        else:\n",
    "            # 3b. Просто обрабатываем данные\n",
    "            features.append(vectorizers[\"\".join(column)].transform(cleared)) # просто преобразем\n",
    "        print(\"  - Processed\\n\")\n",
    "    return features, vectorizers\n",
    "\n",
    "# Делает предсказания для ряда BagOfWords матриц на ряде моделей\n",
    "def modelsPredicts(frames, models):\n",
    "    predictions = []\n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        X = pd.DataFrame(frames[i].toarray())\n",
    "        y = model.predict(X)\n",
    "        predictions.append(y)\n",
    "    return predictions\n",
    "\n",
    "# Добавление не использованных данных + выделение меток\n",
    "def concatenateRemaining(df, predictions, model_columns = MODEL_COLUMNS, cat_cols=CAT_COLS):\n",
    "    # 1. Факторизация категориальных данных\n",
    "    df[cat_cols] = df[cat_cols].apply(lambda x: pd.factorize(x)[0])\n",
    "    X = pd.DataFrame(predictions).T\n",
    "    X = X.set_index(df.index)\n",
    "    for column in MODEL_COLUMNS:\n",
    "        X[column] = df[column]\n",
    "    return X\n",
    "\n",
    "# Вычисление лучших параметров для покрывающей модели (над ост. признаками и результатами выч.)\n",
    "def getCoveringMovelParams(X,y,\n",
    "                       parameters=FOREST_TRAIN_PARAMETERS,\n",
    "                       seed=SEED, \n",
    "                       model=RandomForestClassifier(random_state=SEED),\n",
    "                      ):\n",
    "    grid_search = RandomizedSearchCV(model, \n",
    "                                     param_distributions=parameters, \n",
    "                                     n_iter=15, cv=5, \n",
    "                                     scoring='neg_mean_squared_error', \n",
    "                                     random_state=SEED, \n",
    "                                     verbose = 1)\n",
    "    grid_search.fit(X,y)\n",
    "    return grid_search.best_params_\n",
    "\n",
    "# Обучение локальных моделей, покрывающей модели, векторайзера\n",
    "def trainModel(df, cat_cols=CAT_COLS, max_features=MAX_FEATURES, target_columns=TARGET_COLUMNS, seed=SEED):\n",
    "    # 1. Получение матриц BagOgWords\n",
    "    sparse_frames, vectorizers = preprocessFeatures(train_data)\n",
    "    # 2. Обучаем модель_1[] (модели) для каждой матрицы\n",
    "    models = []\n",
    "    print(\"FITTING LOCAL MODELS\")\n",
    "    i = 1\n",
    "    for fr in sparse_frames:\n",
    "        sgd_clf = SGDClassifier(random_state=seed)\n",
    "#         X = pd.DataFrame(fr.toarray())\n",
    "        X = fr\n",
    "        y = df.is_blocked\n",
    "        sgd_clf.fit(X, y)\n",
    "        models.append(sgd_clf)\n",
    "        print(\"  -fitted {0} from {1}\".format(i, len(sparse_frames)))\n",
    "        i = i + 1;\n",
    "    print()\n",
    "    # 3. Делаем предсказания модель_1[] каждой матрицы\n",
    "    predictions = modelsPredicts(sparse_frames, models)\n",
    "    # 4. Делаем сводную матрицу и добавляем столбцы категорий+доп.данных\n",
    "    X = concatenateRemaining(df, predictions)\n",
    "    y = df['is_blocked']\n",
    "    # 5. По сводной матрице обучаем модель_2\n",
    "    best_params = getCoveringMovelParams(X,y)\n",
    "    covering_model = RandomForestClassifier(**best_params).fit(X,y)\n",
    "    return vectorizers, models, covering_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING TRAIN DATA\n",
      "COLUMN: title\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: description\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: attrs\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: ['title', 'description']\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "FITTING LOCAL MODELS\n",
      "  -fitted 1 from 4\n",
      "  -fitted 2 from 4\n",
      "  -fitted 3 from 4\n",
      "  -fitted 4 from 4\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING TEST DATA\n",
      "COLUMN: title\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: description\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: attrs\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: ['title', 'description']\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### Полный процесс предобработки данных\n",
    "###\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.stats import randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# 1. Обучение модели\n",
    "vectorizers, models, covering_model = trainModel(train_data)\n",
    "\n",
    "# 2. Обработка данных\n",
    "pre_data = preprocessFeatures(test_data, vectorizers)[0]\n",
    "\n",
    "# 3. Предсказания частных моделей\n",
    "priv_predictions = modelsPredicts(pre_data,models)\n",
    "\n",
    "# 4. Получение сводной матрицы\n",
    "X = concatenateRemaining(test_data, priv_predictions)\n",
    "\n",
    "# 5. Предсказание покрывающей модели\n",
    "test_predictions = covering_model.predict(X)\n",
    "pasta = X\n",
    "pasta['is_blocked'] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-db20e93e391a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'is_blocked'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcovering_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;31m# pasta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "X = X.drop('is_blocked', 1)\n",
    "covering_model.score(X, y)\n",
    "# pasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING TRAIN DATA\n",
      "COLUMN: title\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: description\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: attrs\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: ['title', 'description']\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "FITTING LOCAL MODELS\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [10000, 3995803]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-72c5631baf08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;31m# 1. Обучение модели\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mvectorizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcovering_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;31m# 2. Обработка данных\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-70f75883ddda>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(df, cat_cols, max_features, target_columns, seed)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_blocked\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0msgd_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msgd_clf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"  -fitted {0} from {1}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse_frames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    543\u001b[0m                          \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                          sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [10000, 3995803]"
     ]
    }
   ],
   "source": [
    "###\n",
    "### Полный процесс предобработки данных\n",
    "###\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.stats import randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "train = pd.DataFrame.from_csv('data/avito_train.tsv', sep='\\t')\n",
    "test = pd.DataFrame.from_csv('data/avito_test.tsv', sep='\\t')\n",
    "\n",
    "# 1. Обучение модели\n",
    "vectorizers, models, covering_model = trainModel(train)\n",
    "\n",
    "# 2. Обработка данных\n",
    "pre_data = preprocessFeatures(test, vectorizers)[0]\n",
    "\n",
    "# 3. Предсказания частных моделей\n",
    "priv_predictions = modelsPredicts(pre_data,models)\n",
    "\n",
    "# 4. Получение сводной матрицы\n",
    "X = concatenateRemaining(test, priv_predictions)\n",
    "\n",
    "# 5. Предсказание покрывающей модели\n",
    "test_predictions = covering_model.predict(X)\n",
    "# pasta = X\n",
    "# pasta['is_blocked'] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING TRAIN DATA\n",
      "COLUMN: title\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: description\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: attrs\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: ['title', 'description']\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "PROCESSING TEST DATA\n",
      "COLUMN: title\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: description\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: attrs\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: ['title', 'description']\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:    7.5s finished\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "### Полный процесс предобработки данных\n",
    "###\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from scipy.stats import randint\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# 1. Факторизация категориальных данных\n",
    "train_data[CAT_COLS] = train_data[CAT_COLS].apply(lambda x: pd.factorize(x)[0])\n",
    "test_data[CAT_COLS] = test_data[CAT_COLS].apply(lambda x: pd.factorize(x)[0])\n",
    "\n",
    "# 2. Получение матриц BagOgWords\n",
    "# temp = [clearColumn(train_data, test_data, i) for i in TARGET_COLUMNS]\n",
    "sparse_frames_train, vectorizers = preprocessFeatures(train_data)\n",
    "sparse_frames_test = preprocessFeatures(test_data, vectorizers)[0]\n",
    "\n",
    "# 3. Обучаем модель_1[] (модели) для каждой матрицы\n",
    "models = []\n",
    "for fr in sparse_frames_train:\n",
    "    sgd_clf = SGDClassifier(random_state=SEED)\n",
    "    X = pd.DataFrame(fr.toarray())\n",
    "    y = train_data.is_blocked\n",
    "    sgd_clf.fit(X, y)\n",
    "    models.append(sgd_clf)\n",
    "\n",
    "# 4. Делаем предсказания модель_1[] каждой матрицы\n",
    "predictions = []\n",
    "for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    X = pd.DataFrame(sparse_frames_train[i].toarray())\n",
    "    y = model.predict(X)\n",
    "    predictions.append(y)\n",
    "\n",
    "# 5. Делаем сводную матрицу и добавляем столбцы категорий+доп.данных\n",
    "summary = pd.DataFrame(predictions).T\n",
    "summary = summary.set_index(train_data.index)\n",
    "summary['phones_cnt'] = train_data['phones_cnt']\n",
    "summary['price'] = train_data['price']\n",
    "summary['emails_cnt'] = train_data['emails_cnt']\n",
    "summary['urls_cnt'] = train_data['urls_cnt']\n",
    "summary['close_hours'] = train_data['close_hours']\n",
    "summary['category'] = train_data['category']\n",
    "summary['subcategory'] = train_data['subcategory']\n",
    "y_train = train_data['is_blocked']\n",
    "\n",
    "# 6. По сводной матрице обучаем модель_2\n",
    "forest=RandomForestClassifier(n_estimators=10, random_state=SEED)\n",
    "param_grid = {\"max_depth\": randint(low=1, high=15),\n",
    "#               \"max_features\": ['sqrt', 'log2'],\n",
    "#               \"min_samples_leaf\": [4, 8, 16, 32, 64, 128],\n",
    "#               \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "             }\n",
    "\n",
    "grid_search = RandomizedSearchCV(forest, param_distributions=param_grid, \n",
    "                                 n_iter=15, cv=5, scoring='neg_mean_squared_error', random_state=SEED, verbose = 1)\n",
    "grid_search.fit(summary, y_train)\n",
    "forest_params = grid_search.best_params_\n",
    "\n",
    "# 7. По сводной матрице делаем предсказание модель_2\n",
    "clf = RandomForestClassifier(**forest_params)\n",
    "clf.fit(summary, y_train)\n",
    "pasta = summary\n",
    "pasta['is_blocked'] = clf.predict(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99429999999999996"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary = summary.drop('is_blocked', 1)\n",
    "clf.score(summary, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING TEST DATA\n",
      "COLUMN: title\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: description\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: attrs\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n",
      "COLUMN: ['title', 'description']\n",
      "  - Cleared\n",
      "  - Processed\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.96330000000000005"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# полный процесс предсказания по тестовым данным\n",
    "train_data_2 = train_data_raw[10000:20000]\n",
    "\n",
    "train_data_2[CAT_COLS] = train_data_2[CAT_COLS].apply(lambda x: pd.factorize(x)[0])\n",
    "sparse_frames_train_2 = preprocessFeatures(train_data_2, vectorizers)[0]\n",
    "predictions_2 = []\n",
    "for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    X = pd.DataFrame(sparse_frames_train_2[i].toarray())\n",
    "    y = model.predict(X)\n",
    "    predictions_2.append(y)\n",
    "\n",
    "summary_2 = pd.DataFrame(predictions_2).T\n",
    "summary_2 = summary_2.set_index(train_data_2.index)\n",
    "summary_2['phones_cnt'] = train_data_2['phones_cnt']\n",
    "summary_2['price'] = train_data_2['price']\n",
    "summary_2['emails_cnt'] = train_data_2['emails_cnt']\n",
    "summary_2['urls_cnt'] = train_data_2['urls_cnt']\n",
    "summary_2['close_hours'] = train_data_2['close_hours']\n",
    "summary_2['category'] = train_data_2['category']\n",
    "summary_2['subcategory'] = train_data_2['subcategory']\n",
    "y_train_2 = train_data_2['is_blocked']\n",
    "\n",
    "clf.score(summary_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "temp = pd.DataFrame(sparse_frames[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Started:  title\n",
      "##### Started:  description\n"
     ]
    }
   ],
   "source": [
    "params = list()\n",
    "from scipy.stats import randint\n",
    "\n",
    "forest=RandomForestClassifier(n_estimators=10, random_state=SEED)\n",
    "param_grid = {\"max_depth\": randint(low=1, high=15),\n",
    "              \"max_features\": ['sqrt', 'log2'],\n",
    "              \"min_samples_leaf\": [4, 8, 16, 32, 64, 128],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "#              }\n",
    "i = 0\n",
    "for fr in sparse_frames_train[0: 2]:\n",
    "    print(\"##### Started: \", TARGET_COLUMNS[i])\n",
    "    i += 1\n",
    "    X = pd.DataFrame(fr.toarray())\n",
    "    y = df.is_blocked\n",
    "    grid_search = RandomizedSearchCV(forest, param_distributions=param_grid, \n",
    "                                     n_iter=15, cv=5, scoring='neg_mean_squared_error', random_state=45426)\n",
    "    grid_search.fit(X, y)\n",
    "    params.append(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bootstrap': True,\n",
       "  'criterion': 'entropy',\n",
       "  'max_depth': 11,\n",
       "  'max_features': 'sqrt',\n",
       "  'min_samples_leaf': 128},\n",
       " {'bootstrap': True,\n",
       "  'criterion': 'entropy',\n",
       "  'max_depth': 9,\n",
       "  'max_features': 'sqrt',\n",
       "  'min_samples_leaf': 8}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "predictions = []\n",
    "\n",
    "for fr in sparse_frames[0: 1]:  \n",
    "    X = pd.DataFrame(fr.toarray())\n",
    "    y = df.is_blocked\n",
    "    sgd_clf.fit(X, y)\n",
    "    predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.95152424,  0.96051974,  0.959     ,  0.95947974,  0.95997999])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf.predict_proba()\n",
    "# scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "# scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=9322/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_test = pd.DataFrame.from_csv('data/avito_test.tsv', sep='\\t')\n",
    "data_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./scripts')\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_title = pd.DataFrame(preprocessing.frame(train,\"title\").toarray())\n",
    "df_desc = pd.DataFrame(preprocessing.frame(train,\"description\").toarray())\n",
    "df_attrs = pd.DataFrame(preprocessing.frame(train,\"attrs\").toarray())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
